{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom dataset generation, I used a dataset related to food from kaggle named \"Amazon Fine Food Reviews\" (https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews).\n",
    "\n",
    "At first I created a list of some common foods (18 in total) and also a list of some helper words that are likely to be found in sentences related to foods. For example: a sentence with food will most likely have words like tasty, delicious, savoury etc. in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_NAMES = ['gravy','ramen','burger','pizza','pasta','wings',\n",
    "                'coke','sprite','water','fanta','pepsi','seven up', \n",
    "                'biriyani','rice', 'pulao', \n",
    "                'bread', 'flat bread', 'rice bowl']\n",
    "helper_words = ['flavour','flavours','tasty','delicious','juicy','spicy','soft','chewy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate reviews were removed and texts were converted to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = pd.read_csv(\"Reviews.csv\")\n",
    "sentence_df = sentence_df.drop_duplicates(\"Text\")\n",
    "sentence_df[\"Text\"] = sentence_df[\"Text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of reviews with the word 'gravy' in them:  1402\n",
      "Total # of reviews with the word 'gravy' and helper words in them:  255\n",
      "Total # of reviews with the word 'ramen' in them:  1078\n",
      "Total # of reviews with the word 'ramen' and helper words in them:  453\n",
      "Total # of reviews with the word 'burger' in them:  1789\n",
      "Total # of reviews with the word 'burger' and helper words in them:  469\n",
      "Total # of reviews with the word 'pizza' in them:  1945\n",
      "Total # of reviews with the word 'pizza' and helper words in them:  472\n",
      "Total # of reviews with the word 'pasta' in them:  5970\n",
      "Total # of reviews with the word 'pasta' and helper words in them:  1319\n",
      "Total # of reviews with the word 'wings' in them:  454\n",
      "Total # of reviews with the word 'wings' and helper words in them:  140\n",
      "Total # of reviews with the word 'coke' in them:  887\n",
      "Total # of reviews with the word 'coke' and helper words in them:  169\n",
      "Total # of reviews with the word 'sprite' in them:  149\n",
      "Total # of reviews with the word 'sprite' and helper words in them:  39\n",
      "Total # of reviews with the word 'water' in them:  29455\n",
      "Total # of reviews with the word 'water' and helper words in them:  5805\n",
      "Total # of reviews with the word 'fanta' in them:  5809\n",
      "Total # of reviews with the word 'fanta' and helper words in them:  1234\n",
      "Total # of reviews with the word 'pepsi' in them:  384\n",
      "Total # of reviews with the word 'pepsi' and helper words in them:  70\n",
      "Total # of reviews with the word 'seven up' in them:  4\n",
      "Total # of reviews with the word 'seven up' and helper words in them:  1\n",
      "Total # of reviews with the word 'biriyani' in them:  3\n",
      "Total # of reviews with the word 'biriyani' and helper words in them:  2\n",
      "Total # of reviews with the word 'rice' in them:  64708\n",
      "Total # of reviews with the word 'rice' and helper words in them:  10982\n",
      "Total # of reviews with the word 'pulao' in them:  5\n",
      "Total # of reviews with the word 'pulao' and helper words in them:  2\n",
      "Total # of reviews with the word 'bread' in them:  9372\n",
      "Total # of reviews with the word 'bread' and helper words in them:  2418\n",
      "Total # of reviews with the word 'flat bread' in them:  50\n",
      "Total # of reviews with the word 'flat bread' and helper words in them:  11\n",
      "Total # of reviews with the word 'rice bowl' in them:  36\n",
      "Total # of reviews with the word 'rice bowl' and helper words in them:  11\n"
     ]
    }
   ],
   "source": [
    "for food_no,food in enumerate(ENTITY_NAMES):\n",
    "    print(\"Total # of reviews with the word '{}' in them: \".format(food), sentence_df['Text'].str.contains(food).sum())\n",
    "    print(\"Total # of reviews with the word '{}' and helper words in them: \".format(food),(sentence_df['Text'].str.contains(food) & sentence_df['Text'].str.contains('|'.join(helper_words))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every food, first 500 reviews were taken and split into lines. Then only if the line had a food and a helper word in it, it was written in a file called \"food_sentences.txt\". Some minor text formatting was done like removing tags i.e. break. Also for a food, reviews with previous foods were discarded to avoid repitition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_first = 500\n",
    "save_file_name = 'food_sentences.txt'\n",
    "with open(save_file_name,'w') as file:\n",
    "    for food_no,food in enumerate(ENTITY_NAMES):\n",
    "        for sentence in sentence_df['Text'][sentence_df['Text'].str.contains(food)].iloc[:take_first]:\n",
    "            for line in sentence.split('.'):\n",
    "                if any(x in line for x in ENTITY_NAMES) and all(x not in line for x in ENTITY_NAMES[:food_no]) and any(x in line for x in helper_words):\n",
    "                    file.write(line.strip().replace('<br />','')+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sentences saved in the file were then manually annotated using an NER annotator tool and saved in the file \"annotations.json\". The annotator tool that was used is: https://tecoholic.github.io/ner-annotator/ ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4166e6272fbc801281630dafa6c5166de80813f8e620e083364abe1d71aff21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
